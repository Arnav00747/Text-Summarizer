{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5645261-a3b5-4c0a-b314-7fb2d08ca5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10749f45-0d14-4da5-bf56-f53346750a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/12.8 MB 13.0 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.7/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.5 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410ab30b-1946-4752-8459-3ace9bfa5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada0cbf2-f824-4b0c-9da1-78a5c3d2699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c13a52d-9e42-4b5d-a2cc-557cb56ee830",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "In recent years, the field of Natural Language Processing (NLP) has seen tremendous progress.\n",
    "From machine translation to question answering, NLP is powering the latest AI applications.\n",
    "Thanks to the availability of large datasets and powerful computing resources,\n",
    "models like BERT, GPT, and T5 have achieved state-of-the-art performance.\n",
    "This project demonstrates how to build a simple extractive text summarizer using spaCy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12dabe81-ee48-4b36-8b76-a2c90f9f3b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6fcf6cb-d5e2-43d3-86f6-8e08f5411896",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.text.lower() for token in doc \n",
    "          if not token.is_stop and \n",
    "          not token.is_punct and \n",
    "          token.text !='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b10c2b-c453-4bba-a5b6-ba35f1ff6bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recent',\n",
       " 'years',\n",
       " 'field',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'seen',\n",
       " 'tremendous',\n",
       " 'progress',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'question',\n",
       " 'answering',\n",
       " 'nlp',\n",
       " 'powering',\n",
       " 'latest',\n",
       " 'ai',\n",
       " 'applications',\n",
       " 'thanks',\n",
       " 'availability',\n",
       " 'large',\n",
       " 'datasets',\n",
       " 'powerful',\n",
       " 'computing',\n",
       " 'resources',\n",
       " 'models',\n",
       " 'like',\n",
       " 'bert',\n",
       " 'gpt',\n",
       " 't5',\n",
       " 'achieved',\n",
       " 'state',\n",
       " 'art',\n",
       " 'performance',\n",
       " 'project',\n",
       " 'demonstrates',\n",
       " 'build',\n",
       " 'simple',\n",
       " 'extractive',\n",
       " 'text',\n",
       " 'summarizer',\n",
       " 'spacy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c133ec0b-d6fc-4993-af3f-ae57cf474455",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1=[]\n",
    "stopwords = list(STOP_WORDS)\n",
    "allowed_pos = ['ADJ','PROPN','VERB','NOUN']\n",
    "for token in doc:\n",
    "    if token.text in stopwords or token.text in punctuation:\n",
    "        continue\n",
    "    if token.pos_ in allowed_pos:\n",
    "        tokens1.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6876a0e7-d901-41c4-86ed-e77bc565b66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recent',\n",
       " 'years',\n",
       " 'field',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'NLP',\n",
       " 'seen',\n",
       " 'tremendous',\n",
       " 'progress',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'question',\n",
       " 'answering',\n",
       " 'NLP',\n",
       " 'powering',\n",
       " 'latest',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'Thanks',\n",
       " 'availability',\n",
       " 'large',\n",
       " 'datasets',\n",
       " 'powerful',\n",
       " 'computing',\n",
       " 'resources',\n",
       " 'models',\n",
       " 'BERT',\n",
       " 'GPT',\n",
       " 'T5',\n",
       " 'achieved',\n",
       " 'state',\n",
       " 'art',\n",
       " 'performance',\n",
       " 'project',\n",
       " 'demonstrates',\n",
       " 'build',\n",
       " 'simple',\n",
       " 'extractive',\n",
       " 'text',\n",
       " 'summarizer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4afd6c74-361c-4d74-8972-e652e00aa5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1375b956-23e3-4d7b-8c9c-6e8ee27bff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79e227f7-ae51-4ae7-bca0-9af949a0bcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'nlp': 2,\n",
       "         'recent': 1,\n",
       "         'years': 1,\n",
       "         'field': 1,\n",
       "         'natural': 1,\n",
       "         'language': 1,\n",
       "         'processing': 1,\n",
       "         'seen': 1,\n",
       "         'tremendous': 1,\n",
       "         'progress': 1,\n",
       "         'machine': 1,\n",
       "         'translation': 1,\n",
       "         'question': 1,\n",
       "         'answering': 1,\n",
       "         'powering': 1,\n",
       "         'latest': 1,\n",
       "         'ai': 1,\n",
       "         'applications': 1,\n",
       "         'thanks': 1,\n",
       "         'availability': 1,\n",
       "         'large': 1,\n",
       "         'datasets': 1,\n",
       "         'powerful': 1,\n",
       "         'computing': 1,\n",
       "         'resources': 1,\n",
       "         'models': 1,\n",
       "         'like': 1,\n",
       "         'bert': 1,\n",
       "         'gpt': 1,\n",
       "         't5': 1,\n",
       "         'achieved': 1,\n",
       "         'state': 1,\n",
       "         'art': 1,\n",
       "         'performance': 1,\n",
       "         'project': 1,\n",
       "         'demonstrates': 1,\n",
       "         'build': 1,\n",
       "         'simple': 1,\n",
       "         'extractive': 1,\n",
       "         'text': 1,\n",
       "         'summarizer': 1,\n",
       "         'spacy': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35074c75-bdd4-4141-b0bf-4eab34966c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq = max(word_freq.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd5e6bef-0014-4dd6-a34b-ab4e31f3d25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bbda934-4d58-4f9b-91e3-ad6d9a5f4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_freq.keys():\n",
    "    word_freq[word] = word_freq[word]/max_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea8bdb5-290a-429b-a22c-314fa3f78f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'nlp': 1.0,\n",
       "         'recent': 0.5,\n",
       "         'years': 0.5,\n",
       "         'field': 0.5,\n",
       "         'natural': 0.5,\n",
       "         'language': 0.5,\n",
       "         'processing': 0.5,\n",
       "         'seen': 0.5,\n",
       "         'tremendous': 0.5,\n",
       "         'progress': 0.5,\n",
       "         'machine': 0.5,\n",
       "         'translation': 0.5,\n",
       "         'question': 0.5,\n",
       "         'answering': 0.5,\n",
       "         'powering': 0.5,\n",
       "         'latest': 0.5,\n",
       "         'ai': 0.5,\n",
       "         'applications': 0.5,\n",
       "         'thanks': 0.5,\n",
       "         'availability': 0.5,\n",
       "         'large': 0.5,\n",
       "         'datasets': 0.5,\n",
       "         'powerful': 0.5,\n",
       "         'computing': 0.5,\n",
       "         'resources': 0.5,\n",
       "         'models': 0.5,\n",
       "         'like': 0.5,\n",
       "         'bert': 0.5,\n",
       "         'gpt': 0.5,\n",
       "         't5': 0.5,\n",
       "         'achieved': 0.5,\n",
       "         'state': 0.5,\n",
       "         'art': 0.5,\n",
       "         'performance': 0.5,\n",
       "         'project': 0.5,\n",
       "         'demonstrates': 0.5,\n",
       "         'build': 0.5,\n",
       "         'simple': 0.5,\n",
       "         'extractive': 0.5,\n",
       "         'text': 0.5,\n",
       "         'summarizer': 0.5,\n",
       "         'spacy': 0.5})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88bf43f4-fa25-4033-a8bd-1d5b7fa452ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af25d734-a9ca-42ad-943d-8eef71576d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nIn recent years, the field of Natural Language Processing (NLP) has seen tremendous progress.\\n',\n",
       " 'From machine translation to question answering, NLP is powering the latest AI applications.\\n',\n",
       " 'Thanks to the availability of large datasets and powerful computing resources,\\nmodels like BERT, GPT, and T5 have achieved state-of-the-art performance.\\n',\n",
       " 'This project demonstrates how to build a simple extractive text summarizer using spaCy.\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3305923a-3eaa-46ac-8173-b030264d7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "recent\n",
      "years,\n",
      "the\n",
      "field\n",
      "of\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "(NLP)\n",
      "has\n",
      "seen\n",
      "tremendous\n",
      "progress.\n",
      "From\n",
      "machine\n",
      "translation\n",
      "to\n",
      "question\n",
      "answering,\n",
      "NLP\n",
      "is\n",
      "powering\n",
      "the\n",
      "latest\n",
      "AI\n",
      "applications.\n",
      "Thanks\n",
      "to\n",
      "the\n",
      "availability\n",
      "of\n",
      "large\n",
      "datasets\n",
      "and\n",
      "powerful\n",
      "computing\n",
      "resources,\n",
      "models\n",
      "like\n",
      "BERT,\n",
      "GPT,\n",
      "and\n",
      "T5\n",
      "have\n",
      "achieved\n",
      "state-of-the-art\n",
      "performance.\n",
      "This\n",
      "project\n",
      "demonstrates\n",
      "how\n",
      "to\n",
      "build\n",
      "a\n",
      "simple\n",
      "extractive\n",
      "text\n",
      "summarizer\n",
      "using\n",
      "spaCy.\n"
     ]
    }
   ],
   "source": [
    "sent_score = {}\n",
    "for sent in sent_token:\n",
    "    for word in sent.split():\n",
    "        if word.lower() in word_freq.keys():\n",
    "            if sent not in sent_score.keys():\n",
    "                sent_score[sent] = word_freq[word]\n",
    "            else:\n",
    "                sent_score[sent] +=word_freq[word]\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aea5622e-8703-4a9a-93eb-7f8f598e1a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\nIn recent years, the field of Natural Language Processing (NLP) has seen tremendous progress.\\n': 2.0,\n",
       " 'From machine translation to question answering, NLP is powering the latest AI applications.\\n': 2.5,\n",
       " 'Thanks to the availability of large datasets and powerful computing resources,\\nmodels like BERT, GPT, and T5 have achieved state-of-the-art performance.\\n': 4.0,\n",
       " 'This project demonstrates how to build a simple extractive text summarizer using spaCy.\\n': 3.5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d13b5874-24cc-44c9-aab4-b3a7d992bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0df1028a-002f-4f30-83f4-93a1186a5dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn recent years, the field of Natural Langua...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From machine translation to question answering...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks to the availability of large datasets a...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This project demonstrates how to build a simpl...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Score\n",
       "0  \\nIn recent years, the field of Natural Langua...    2.0\n",
       "1  From machine translation to question answering...    2.5\n",
       "2  Thanks to the availability of large datasets a...    4.0\n",
       "3  This project demonstrates how to build a simpl...    3.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(sent_score.items()),columns=['Sentence','Score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b3ed2b8-6787-4816-8528-05d8e3b238d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn recent years, the field of Natural Langua...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From machine translation to question answering...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks to the availability of large datasets a...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This project demonstrates how to build a simpl...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Score\n",
       "0  \\nIn recent years, the field of Natural Langua...    2.0\n",
       "1  From machine translation to question answering...    2.5\n",
       "2  Thanks to the availability of large datasets a...    4.0\n",
       "3  This project demonstrates how to build a simpl...    3.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(sent_score.items()),columns=['Sentence','Score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff0c4cd7-0c87-4cef-ae66-e730260dc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31e0fc71-187d-4971-8342-31447c9dab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences =3\n",
    "n = nlargest(num_sentences,sent_score,key=sent_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "548e5ddc-cb9a-4747-b57a-82ae55fe88e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks to the availability of large datasets and powerful computing resources,\\nmodels like BERT, GPT, and T5 have achieved state-of-the-art performance.\\n This project demonstrates how to build a simple extractive text summarizer using spaCy.\\n From machine translation to question answering, NLP is powering the latest AI applications.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c573b8a4-4187-4c57-b2cb-6887700cfd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "===== Summary =====\n",
      "Thanks to the availability of large datasets and powerful computing resources,\n",
      "models like BERT, GPT, and T5 have achieved state-of-the-art performance.\n",
      " \n",
      "In recent years, the field of Natural Language Processing (NLP) has seen tremendous progress.\n",
      " From machine translation to question answering, NLP is powering the latest AI applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required model if not done already\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import libraries\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "In recent years, the field of Natural Language Processing (NLP) has seen tremendous progress.\n",
    "From machine translation to question answering, NLP is powering the latest AI applications.\n",
    "Thanks to the availability of large datasets and powerful computing resources,\n",
    "models like BERT, GPT, and T5 have achieved state-of-the-art performance.\n",
    "This project demonstrates how to build a simple extractive text summarizer using spaCy.\n",
    "\"\"\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = {}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in STOP_WORDS and word.text.lower() not in punctuation:\n",
    "        word_text = word.text.lower()\n",
    "        word_frequencies[word_text] = word_frequencies.get(word_text, 0) + 1\n",
    "\n",
    "# Normalize frequencies\n",
    "max_freq = max(word_frequencies.values())\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] /= max_freq\n",
    "\n",
    "# Score each sentence\n",
    "sentence_scores = {}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies:\n",
    "            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_frequencies[word.text.lower()]\n",
    "\n",
    "# Select top N sentences for summary\n",
    "summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "final_summary = ' '.join([sent.text for sent in summary_sentences])\n",
    "\n",
    "# Output\n",
    "print(\"===== Summary =====\")\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db253e-f0af-4ba5-8f8c-7d6ca331df1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
